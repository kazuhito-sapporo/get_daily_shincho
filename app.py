import streamlit as st
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import os

# --- ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ---
SAVE_DIR = "./saved_articles"
os.makedirs(SAVE_DIR, exist_ok=True)

# --- ãƒ‡ã‚¤ãƒªãƒ¼æ–°æ½®ï¼šè¨˜äº‹ä¸€è¦§å–å¾—ï¼ˆBeautifulSoupï¼‰ ---
def get_daily_shincho_articles():
    url = "https://www.dailyshincho.jp/"
    res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(res.content, "html.parser")
    elements = soup.select("a:has(p.c-list-article__caption)")
    articles = []
    for el in elements:
        title_el = el.select_one("p.c-list-article__caption")
        title = title_el.text.strip() if title_el else ""
        link = el.get("href")
        if title and link and link.startswith("https"):
            articles.append({"title": title, "url": link})
    return articles

# --- æœ¬æ–‡å–å¾—ï¼ˆBeautifulSoupï¼‰---
def get_article_body_bs4(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.content, "html.parser")
        body_el = soup.select_one("article")
        if not body_el:
            return "ï¼ˆæœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰"
        paragraphs = [p.text.strip() for p in body_el.find_all("p") if p.text.strip()]
        return "\n".join(paragraphs)
    except Exception as e:
        return f"ï¼ˆæœ¬æ–‡å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ï¼‰"

# --- Streamlit UI ---
st.set_page_config(page_title="è¤‡æ•°ãƒ¡ãƒ‡ã‚£ã‚¢è¨˜äº‹ãƒ“ãƒ¥ãƒ¼ã‚¢", layout="centered")
st.title("ğŸ—ï¸ è¤‡æ•°ãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œè¨˜äº‹ãƒ“ãƒ¥ãƒ¼ã‚¢ï¼ˆCloudå¯¾å¿œï¼‰")

media = st.radio("ğŸ“° ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’é¸æŠ", ["ãƒ‡ã‚¤ãƒªãƒ¼æ–°æ½®", "æ–‡æ˜¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³"], horizontal=True)

if media == "ãƒ‡ã‚¤ãƒªãƒ¼æ–°æ½®":
    if st.button("ğŸ—ï¸ æ–°æ½®è¨˜äº‹ä¸€è¦§ã‚’å–å¾—"):
        with st.spinner("è¨˜äº‹ä¸€è¦§ã‚’å–å¾—ä¸­..."):
            st.session_state.articles = get_daily_shincho_articles()
            st.session_state.body = ""
            st.session_state.selected_title = ""

    if "articles" in st.session_state and st.session_state.articles:
        titles = [a["title"] for a in st.session_state.articles]
        selected_title = st.selectbox("ğŸ“° è¨˜äº‹ã‚’é¸æŠ", titles)
        selected_article = next((a for a in st.session_state.articles if a["title"] == selected_title), None)

        if selected_article:
            st.markdown(f"ğŸ”— [è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’é–‹ã]({selected_article['url']})")
            if st.button("ğŸ“– æœ¬æ–‡ã‚’è¡¨ç¤º"):
                with st.spinner("æœ¬æ–‡ã‚’å–å¾—ä¸­..."):
                    st.session_state.body = get_article_body_bs4(selected_article["url"])
                st.session_state.selected_title = selected_title

elif media == "æ–‡æ˜¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³":
    url = st.text_input("ğŸ”— è¨˜äº‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: https://bunshun.jp/articles/-/XXXXXï¼‰")
    if url:
        if st.button("ğŸ“– æœ¬æ–‡ã‚’è¡¨ç¤º"):
            with st.spinner("æœ¬æ–‡ã‚’å–å¾—ä¸­..."):
                st.session_state.body = get_article_body_bs4(url)
            st.session_state.selected_title = "æ–‡æ˜¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³è¨˜äº‹"

# --- æœ¬æ–‡è¡¨ç¤ºï¼†ä¿å­˜ ---
if "body" in st.session_state and st.session_state.body:
    st.subheader("ğŸ“„ è¨˜äº‹æœ¬æ–‡")
    st.write(st.session_state.body)

    if st.button("ğŸ’¾ ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜"):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_title = st.session_state.selected_title[:30].replace(" ", "_").replace("/", "_").replace(":", "_")
        filename = os.path.join(SAVE_DIR, f"{safe_title}_{timestamp}.txt")
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"{st.session_state.selected_title}\n{url if media == 'æ–‡æ˜¥ã‚ªãƒ³ãƒ©ã‚¤ãƒ³' else selected_article['url']}\n\n{st.session_state.body}")
        st.success(f"âœ… ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
